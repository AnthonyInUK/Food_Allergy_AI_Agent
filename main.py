from graph_logic import query_with_graph
import streamlit as st
from dotenv import load_dotenv
from langchain_community.chat_message_histories import StreamlitChatMessageHistory

load_dotenv()

st.set_page_config(page_title="Food Allergy AI Agent", layout="wide")

# 1. åˆå§‹åŒ–è®°å¿†ã€å¤„ç†çŠ¶æ€å’Œè¯­ä¹‰ç¼“å­˜
msgs = StreamlitChatMessageHistory(key="messages")
if "last_processed_file" not in st.session_state:
    st.session_state.last_processed_file = None
if "response_cache" not in st.session_state:
    st.session_state.response_cache = {}

st.title("ğŸ¥— Food Allergy AI Agent")
st.markdown("ä¸Šä¼ é£Ÿå“å›¾ç‰‡æˆ–ç›´æ¥æé—®ï¼Œæˆ‘ä¼šå¸®ä½ æ£€æŸ¥è¿‡æ•åŸã€‚")

with st.sidebar:
    st.header("âš™ï¸ è®¾ç½®")
    language = st.selectbox(
        "é€‰æ‹©å›å¤è¯­è¨€ / Language",
        ["è‡ªåŠ¨è¯†åˆ« (Auto)", "ç®€ä½“ä¸­æ–‡", "English", "FranÃ§ais"],
        index=0
    )
    st.session_state.target_language = language

# 2. ä¾§è¾¹æ ï¼šä¸Šä¼ åŠŸèƒ½
with st.sidebar:
    st.header("å›¾ç‰‡è¯†åˆ«")
    uploaded_file = st.file_uploader(
        "ä¸Šä¼ é£Ÿå“åŒ…è£…æˆ–é…æ–™è¡¨å›¾ç‰‡",
        type=["jpg", "jpeg", "png"],
        key="sidebar_uploader"
    )
    if uploaded_file:
        st.image(uploaded_file, caption="å¾…å¤„ç†å›¾ç‰‡", use_container_width=True)

        if uploaded_file.name != st.session_state.last_processed_file:
            if st.button("å¼€å§‹è¯†åˆ«è¿‡æ•åŸ"):
                with st.chat_message("assistant"):
                    with st.spinner("è§†è§‰è¯†åˆ«ä¸­..."):
                        try:
                            image_bytes = uploaded_file.getvalue()
                            response = ""
                            # å¤„ç†æµå¼ç”Ÿæˆå™¨
                            for step in query_with_graph("è¯·è¯†åˆ«è¿™å¼ å›¾ç‰‡ä¸­çš„é£Ÿå“åç§°ï¼Œå¹¶æ ¹æ®æ•°æ®åº“æŸ¥è¯¢å…¶è¿‡æ•åŸä¿¡æ¯ã€‚", image_bytes=image_bytes):
                                if step["node"] == "end":
                                    response = step["generation"]

                            msgs.add_user_message("ğŸ“¸ [ç”¨æˆ·ä¸Šä¼ äº†å›¾ç‰‡]")
                            msgs.add_ai_message(response)
                            st.session_state.last_processed_file = uploaded_file.name
                            st.rerun()
                        except Exception as e:
                            st.error(f"è¯†åˆ«å¤±è´¥: {str(e)}")

# 3. ä¸»ç•Œé¢æ¸²æŸ“å†å²è®°å½•
for msg in msgs.messages:
    role = "user" if msg.type == "human" else "assistant"
    with st.chat_message(role):
        st.markdown(msg.content)

# 4. åº•éƒ¨æ–‡å­—é—®ç­”å…¥å£ (å¸¦æ€è€ƒè¿‡ç¨‹å±•ç¤ºä¸ç¼“å­˜)
if prompt := st.chat_input("ä¾‹å¦‚ï¼šæé”¦è®°æœ‰å“ªäº›ä¸å«å¤§è±†çš„é…±ï¼Ÿ"):
    st.chat_message("user").markdown(prompt)
    msgs.add_user_message(prompt)

    with st.chat_message("assistant"):
        with st.status("ğŸ” æ­£åœ¨æ€è€ƒ...", expanded=True) as status:
            final_response = ""
            query_gen = query_with_graph(prompt)

            # ç¬¬ä¸€æ­¥ï¼šæ˜¾å¼å‘ŠçŸ¥ç”¨æˆ·æ­£åœ¨æŸ¥ç¼“å­˜ï¼ˆå¢åŠ ä¸“ä¸šæ„Ÿï¼‰
            st.write("ğŸ“‚ æ­£åœ¨è¿›è¡Œè¯­ä¹‰ç¼“å­˜æ¯”å¯¹...")

            try:
                # è¿è¡Œç”Ÿæˆå™¨
                for step in query_gen:
                    node = step.get("node")

                    if node == "contextualize_question":
                        refined_q = step.get("refined_q", prompt)
                        st.write(f"ğŸš¦ è¯†åˆ«åˆ°æ‚¨çš„æ„å›¾ä¸º: **{refined_q}**")

                    # ã€æ ¸å¿ƒä¿®å¤ã€‘ï¼šç»Ÿä¸€ä½¿ç”¨ response_cache è¿™ä¸ª Key
                    elif node == "cache_hit":
                        st.success("âœ¨ **[è¯­ä¹‰çº§å‘½ä¸­]** å‘ç°å†å²æè®®æ„å›¾ï¼Œæ­£åœ¨é—ªç°ç­”æ¡ˆ...")

                    elif node == "route_question":
                        st.write("ğŸš¦ æ­£åœ¨åˆ†æé—®é¢˜åˆ†å‘è·¯å¾„...")
                    elif node == "retrieve":
                        st.write("ğŸ“š æ­£åœ¨æ£€ç´¢æœ¬åœ°å‘é‡æ•°æ®åº“...")
                    elif node == "sql_agent":
                        st.write("ğŸ“Š æ­£åœ¨æ‰§è¡Œ SQL ç²¾å‡†æ•°æ®åº“æŸ¥è¯¢...")
                    elif node == "grade_documents":
                        st.write("âš–ï¸ æ­£åœ¨è¯„ä¼°èµ„æ–™ç›¸å…³æ€§...")
                    elif node == "web_search":
                        st.write("ğŸŒ æœ¬åœ°èµ„æ–™ä¸è¶³ï¼Œæ­£åœ¨å¯åŠ¨è”ç½‘æœç´¢...")
                    elif node == "hallucination_grader":
                        st.write("ğŸ•µï¸ æ­£åœ¨è¿›è¡Œäº‹å®æ ¸æŸ¥...")
                    elif node == "answer_grader":
                        st.write("âœ… æ­£åœ¨ç¡®è®¤å›ç­”æ˜¯å¦è§£å†³äº†æ‚¨çš„é—®é¢˜...")

                    if node == "end":
                        final_response = step["generation"]
                        duration = step["duration"]
                        status.update(
                            label=f"âœ… æ€è€ƒå®Œæˆ (è€—æ—¶ {duration:.2f}ç§’)", state="complete", expanded=False)
            except Exception as e:
                st.error(f"é€»è¾‘æ‰§è¡Œå‡ºé”™: {str(e)}")

        # æ€è€ƒå®Œæˆåï¼Œæ˜¾ç¤ºæœ€ç»ˆå›ç­”
        if final_response:
            st.markdown(final_response)
            msgs.add_ai_message(final_response)
