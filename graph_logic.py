import os
import time
import base64
from typing import List, TypedDict, Annotated, Union
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
import streamlit as st
from langgraph.graph import END, StateGraph, START

from agent_logic import get_sql_agent, get_db, get_llm, query_text as sql_query_text

load_dotenv()

# --- 1. èµ„æºç¼“å­˜æ±  ---


@st.cache_resource
def get_fast_llm():
    return ChatOpenAI(model="gpt-4o-mini", temperature=0)


@st.cache_resource
def get_vectorstore():
    embeddings = OpenAIEmbeddings()
    return Chroma(persist_directory="data/chroma_db", embedding_function=embeddings)


# --- 2. ç»“æ„åŒ–è¾“å‡º Schema ---
route_schema = {
    "name": "route_query",
    "description": "åˆ¤å®šç”¨æˆ·æŸ¥è¯¢çš„æ„å›¾å’Œé¢†åŸŸèŒƒå›´",
    "parameters": {
        "type": "object",
        "properties": {
            "datasource": {"type": "string", "enum": ["sql_db", "vector_db", "off_topic"]}
        },
        "required": ["datasource"]
    }
}

grade_schema = {
    "name": "grade",
    "parameters": {
        "type": "object",
        "properties": {"score": {"type": "string", "enum": ["yes", "no"]}},
        "required": ["score"]
    }
}

# --- 3. çŠ¶æ€å®šä¹‰ ---


class GraphState(TypedDict):
    question: str
    generation: str
    web_search: str
    documents: List[str]
    router_decision: str
    hallucination_score: str
    answer_score: str
    retry_count: int
    target_language: str

# --- 4. èŠ‚ç‚¹é€»è¾‘ ---


def contextualize_question(state):
    print("--- ğŸš¦ æ­£åœ¨è¿›è¡Œä¸Šä¸‹æ–‡è¡¥å…¨ ---")
    question = state["question"]

    msgs = StreamlitChatMessageHistory(key="messages")
    history = msgs.messages[:-1][-5:] if len(msgs.messages) > 1 else []

    if not history:
        return {"question": question}

    llm = get_fast_llm()
    system_instruction = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®é¢˜é‡å†™ä¸“å®¶ã€‚
        ä½ çš„ä»»åŠ¡æ˜¯ï¼šæ ¹æ®å¯¹è¯å†å²ï¼Œå°†ç”¨æˆ·æœ€æ–°çš„æé—®æ”¹å†™ä¸ºä¸€ä¸ªã€å®Œå…¨ç‹¬ç«‹ã€æ— æ­§ä¹‰ã€‘çš„é—®é¢˜ã€‚
        
        ã€æ ¸å¿ƒè¦æ±‚ã€‘
        1. æ¶ˆé™¤ä»£è¯ï¼šå¿…é¡»å°†â€œè¿™ä¸ªâ€ã€â€œå®ƒâ€ã€â€œthisâ€ã€â€œitâ€ç­‰è¯ï¼Œæ›¿æ¢ä¸ºå†å²å¯¹è¯ä¸­æåˆ°çš„å…·ä½“é£Ÿå“åç§°æˆ–å“ç‰Œã€‚
        2. è·¨è¯­è¨€å¯¹é½ï¼šå³ä½¿å†å²æ˜¯ä¸­æ–‡è€Œå½“å‰æé—®æ˜¯è‹±æ–‡ï¼ˆæˆ–åä¹‹ï¼‰ï¼Œä½ ä¹Ÿå¿…é¡»å‡†ç¡®æå–å“ç‰Œåï¼ˆå¦‚æé”¦è®°/Lee Kum Keeï¼‰å¹¶åµŒå…¥æ–°é—®é¢˜ä¸­ã€‚
        3. ä¸¥ç¦å·æ‡’ï¼šä¸¥ç¦è¾“å‡ºç±»ä¼¼ "this product" æˆ– "the sauce" è¿™ç§ä¾ç„¶æ¨¡ç³Šçš„è¯ï¼Œå¿…é¡»è¯´å‡ºå…¨åã€‚
        4. ä¿æŒåŸæ„ï¼šä¸è¦å›ç­”é—®é¢˜ï¼Œåªéœ€é‡å†™æé—®ã€‚ç›´æ¥è¾“å‡ºé‡å†™åçš„ç»“æœã€‚
        """

    # è·å–ç›®æ ‡è¯­è¨€
    target_lang = state.get("target_language", "è‡ªåŠ¨è¯†åˆ« (Auto)")

    # æ ¸å¿ƒä¼˜åŒ–ç‚¹ï¼šå¦‚æœæˆ‘ä»¬è¦ç”Ÿæˆ Keyï¼Œä½¿ç”¨ä¸€ç§æå…¶æ­»æ¿çš„æ ¼å¼
    if target_lang == "Normalized_Key":
        system_instruction = """ä½ æ˜¯ä¸€ä¸ªå¤šè¯­è¨€å®ä½“å¯¹é½ä¸“å®¶ã€‚
        ä½ çš„ä»»åŠ¡ï¼šä»å¯¹è¯å†å²ä¸­æå–æ ¸å¿ƒæ„å›¾ï¼Œå¹¶å°†å…¶ã€å¼ºåˆ¶ç»Ÿä¸€ç¿»è¯‘ä¸ºè‹±æ–‡ã€‘ã€‚
        
        å¿…é¡»è¾“å‡ºæ­¤æ ¼å¼ï¼š[æ„å›¾]|[è‹±æ–‡å“ç‰Œ]|[è‹±æ–‡äº§å“å]
        æ„å›¾åˆ†ç±»ï¼šAllergyCheck, InfoSearch, Appearance
        ç¿»è¯‘ç¤ºä¾‹ï¼š
        - â€œæé”¦è®°â€ -> "Lee Kum Kee"
        - â€œè€æŠ½â€ -> "Dark Soy Sauce"
        - â€œèƒ½åƒå—â€ -> "AllergyCheck"
        
        è¾“å‡ºç¤ºä¾‹ï¼šAllergyCheck|Lee Kum Kee|Dark Soy Sauce
        ä¸¥ç¦è¾“å‡ºä»»ä½•ä¸­æ–‡æˆ–å¤šä½™å•è¯ã€‚"""
    else:
        # ç”¨äº UI å±•ç¤ºçš„æç¤ºè¯ä¿æŒåŸæœ‰çš„çµæ´»æ€§
        system_instruction = "ä½ æ˜¯ä¸€ä¸ªé—®é¢˜é‡å†™ä¸“å®¶ã€‚æ ¹æ®å¯¹è¯å†å²ï¼Œå°†æé—®æ”¹å†™ä¸ºç‹¬ç«‹çš„å®Œæ•´æé—®ã€‚å¤„ç†ä»£è¯æŒ‡ä»£ã€‚"

    if target_lang == "English":
        system_instruction += " è¯·åŠ¡å¿…ä½¿ç”¨ã€è‹±æ–‡ã€‘é‡å†™é—®é¢˜ã€‚"
    elif target_lang == "FranÃ§ais":
        system_instruction += " è¯·åŠ¡å¿…ä½¿ç”¨ã€æ³•è¯­ã€‘é‡å†™é—®é¢˜ã€‚"
    elif target_lang == "è‡ªåŠ¨è¯†åˆ« (Auto)":
        system_instruction += " è¯·ä¿æŒä¸ç”¨æˆ·æé—®ç›¸åŒçš„è¯­è¨€ã€‚"
    else:
        system_instruction += " è¯·ä½¿ç”¨ã€ä¸­æ–‡ã€‘é‡å†™é—®é¢˜ã€‚"

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_instruction),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{question}")
    ])
    res = (prompt | llm).invoke({"history": history, "question": question})
    print(f"ã€é—®é¢˜è¡¥å…¨ç»“æœã€‘: {res.content}")
    return {"question": res.content}


def route_question(state):
    """ã€100% è¿˜åŸä½ æœ€æ»¡æ„çš„é«˜ç²¾åº¦è·¯ç”±æç¤ºè¯ã€‘"""
    print("--- æ™ºèƒ½è·¯ç”±ä¸å®‰å…¨ç½‘å…³ ---")
    llm = get_fast_llm()
    structured_llm = llm.with_structured_output(route_schema)

    system = """ä½ æ˜¯ä¸€ä¸ªé«˜ç²¾åº¦çš„é£Ÿå“å®‰å…¨è·¯ç”±ä¸“å®¶ã€‚
    ã€æ ¸å¿ƒåˆ†å‘è§„åˆ™ - å¿…é¡»æ­»å®ˆã€‘
    1. å¿…é¡»é€‰ 'sql_db' (ç²¾å‡†æŸ¥è¯¢)ï¼š
       - é—®é¢˜ä¸­åŒ…å«ã€å…·ä½“å“ç‰Œã€‘ï¼ˆå¦‚ï¼šæé”¦è®°ã€Lee Kum Keeã€åº·å¸ˆå‚…ã€æµ·å¤©ç­‰ï¼‰ï¼Œå¿…é¡»é€‰æ­¤é¡¹ã€‚
       - å³ä¾¿æ˜¯é—®â€œèƒ½åƒå—â€ã€â€œå«å¤§è±†å—â€ï¼Œåªè¦æœ‰å“ç‰Œï¼Œä¹Ÿå¿…é¡»è·¯ç”±åˆ° 'sql_db'ã€‚
       - é—®é¢˜æ¶‰åŠã€ç‰¹å®šäº§å“åã€‘ï¼ˆå¦‚ï¼šè€æŠ½ã€é…±æ²¹ã€ç•ªèŒ„é…±ï¼‰ã€‚
       - é—®é¢˜è¦æ±‚çœ‹ã€å›¾ç‰‡/å¤–è§‚/é•¿ä»€ä¹ˆæ ·/åŒ…è£…ã€‘ã€‚
       - æ¶‰åŠç»Ÿè®¡æˆ–åˆ—è¡¨ï¼ˆå¦‚ï¼šæœ‰å“ªäº›ä¸å«å¤§è±†çš„é…±ï¼Ÿï¼‰ã€‚
    
    2. å¿…é¡»é€‰ 'vector_db' (å¸¸è¯†/å»ºè®®)ï¼š
    
     - ä»…å½“é—®é¢˜æ˜¯ã€ä¸€èˆ¬æ€§çŸ¥è¯†ã€‘ï¼ˆå¦‚ï¼šä»€ä¹ˆæ˜¯é¢ç­‹ï¼Ÿï¼‰æˆ–ã€å®Œå…¨æ²¡æœ‰å“ç‰Œåã€‘çš„æ¨¡ç³Šå’¨è¯¢ã€‚
       - å…³äºã€æˆåˆ†çŸ¥è¯†ã€‘ï¼ˆå¦‚ï¼šä»€ä¹ˆæ˜¯é¢ç­‹ï¼Ÿé˜²è…å‰‚æœ‰å®³å—ï¼Ÿï¼‰ã€‚
       - åªè¦æ¶‰åŠé£Ÿå“ã€è¿‡æ•ã€èº«ä½“å®‰å…¨ï¼Œå¿…é¡»åœ¨è¿™ 1 å’Œ 2 ä¹‹é—´é€‰æ‹©ï¼
    
    3. å¿…é¡»é€‰ 'off_topic' (ä¸¥æ ¼æ‹¦æˆª)ï¼š
       - ä»…å½“é—®é¢˜ä¸ã€é£Ÿå“ã€è¿‡æ•ã€å“ç‰Œã€è¥å…»ã€å®‰å…¨ã€‘å®Œå…¨æ— å…³æ—¶ã€‚
       - ä¾‹å¦‚ï¼šé—®ä»£ç ã€æ”¿æ²»ã€é—²èŠã€é—®å¤©æ°”ã€é—®æ•°å­¦é¢˜ç­‰ã€‚
    """
    # æˆ‘ä»¬æŠŠè¿™ä¸ªä½œä¸ºâ€œç¡¬æç¤ºâ€å¡ç»™è·¯ç”±å™¨
    question_to_route = state["question"]
    # æå–å“ç‰Œåå•ï¼ˆå¯ä»¥åŠ¨æ€è·å–ï¼Œè¿™é‡Œç¤ºä¾‹å‡ ä¸ªæ ¸å¿ƒå“ç‰Œï¼‰
    known_brands = ["lee kum kee", "æé”¦è®°", "haday", "æµ·å¤©", "master kong", "åº·å¸ˆå‚…"]

    # å¦‚æœé—®é¢˜é‡Œæœ‰è¿™äº›è¯ï¼Œæˆ‘ä»¬åœ¨é—®é¢˜æœ«å°¾å¼ºè¡ŒåŠ ä¸ªæç¤º
    if any(brand in question_to_route.lower() for brand in known_brands):
        question_to_route += " (Note: This question contains a specific brand, prioritize structured data source.)"

    res = (ChatPromptTemplate.from_messages(
        [("system", system), ("human", "{question}")]) | structured_llm).invoke({"question": state["question"]})
    decision = res["datasource"] if isinstance(res, dict) else res.datasource
    return {"router_decision": decision}


def handle_off_topic(state):
    print("--- ğŸš« æ‹¦æˆªéæ³•è¯·æ±‚ ---")
    target_lang = state.get("target_language", "è‡ªåŠ¨è¯†åˆ« (Auto)")
    msg = "æŠ±æ­‰ï¼Œæˆ‘æ˜¯ä¸€åä¸“ä¸šçš„é£Ÿå“è¿‡æ•åŠ©æ‰‹ã€‚æˆ‘åªèƒ½å›ç­”ä¸é£Ÿå“æˆåˆ†ã€è¿‡æ•åŸã€é£Ÿå“å“ç‰Œä»¥åŠé£Ÿå“å®‰å…¨ç›¸å…³çš„é—®é¢˜ã€‚"
    if target_lang == "English":
        msg = "Sorry, I am a professional Food Allergy Assistant. I can only answer questions related to food ingredients, allergens, brands, and safety."
    elif target_lang == "FranÃ§ais":
        msg = "DÃ©solÃ©, je suis un assistant professionnel pour les allergies alimentaires. Je ne peux rÃ©pondre qu'aux questions relatives aux ingrÃ©dients, aux allergÃ¨nes, aux marques et Ã  la sÃ©curitÃ© alimentaire."
    return {"generation": msg}


def call_sql_agent(state):
    print("--- å¯åŠ¨ SQL Agent ---")
    response, _ = sql_query_text(state["question"])
    return {"generation": response}


def retrieve(state):
    print("--- æ£€ç´¢æœ¬åœ°çŸ¥è¯†åº“ ---")
    docs = get_vectorstore().similarity_search(state["question"], k=3)
    doc_texts = [
        f"å†…å®¹: {d.page_content}\næ¥æº: {d.metadata.get('source', 'æœ¬åœ°çŸ¥è¯†åº“')}" for d in docs]
    return {"documents": doc_texts}


def grade_documents(state):
    if not state.get("documents"):
        return {"web_search": "Yes"}
    llm = get_fast_llm()
    res = (ChatPromptTemplate.from_messages([("system", "åˆ¤æ–­èµ„æ–™æ˜¯å¦è¶³ä»¥å›ç­”é—®é¢˜ã€‚"), ("human", "é—®é¢˜: {question} \nèµ„æ–™: {documents}")]) | llm.with_structured_output(
        grade_schema)).invoke({"question": state["question"], "documents": ' '.join(state["documents"])})
    score = res["score"] if isinstance(res, dict) else res.score
    return {"web_search": "No" if score == "yes" else "Yes"}


def generate(state):
    print("--- ç”Ÿæˆå›ç­” ---")
    retry_count = state.get("retry_count", 0) + 1
    target_lang = state.get("target_language", "è‡ªåŠ¨è¯†åˆ« (Auto)")
    lang_instruction = ""
    if target_lang == "ç®€ä½“ä¸­æ–‡":
        lang_instruction = "è¯·ä½¿ç”¨ã€ç®€ä½“ä¸­æ–‡ã€‘å›ç­”ã€‚"
    elif target_lang == "English":
        lang_instruction = "Please reply in ã€Englishã€‘."
    elif target_lang == "FranÃ§ais":
        lang_instruction = "RÃ©pondez en ã€FranÃ§aisã€‘."
    else:
        lang_instruction = "è¯·ä½¿ç”¨ç”¨æˆ·æé—®çš„è¯­è¨€å›ç­”ã€‚"

    llm = get_llm()
    prompt = ChatPromptTemplate.from_template(
        f"ä½ æ˜¯ä¸€ä¸ªé£Ÿå“è¿‡æ•ä¸“å®¶ã€‚\n{lang_instruction}\nã€é‡è¦ã€‘å¿…é¡»åœ¨æœ«å°¾åˆ—å‡ºå‚è€ƒæ¥æºã€‚\nèµ„æ–™: {{documents}}\né—®é¢˜: {{question}}")
    response = (prompt | llm).invoke(
        {"documents": state["documents"], "question": state["question"]})
    return {"generation": response.content, "retry_count": retry_count}


def web_search(state):
    print("--- è§¦å‘è”ç½‘æœç´¢ ---")
    from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper
    search = TavilySearchResults(api_wrapper=TavilySearchAPIWrapper(
        tavily_api_key=os.getenv("TAVILY_API_KEY")), k=3)
    results = search.invoke({"query": state["question"]})
    web_docs = [f"å†…å®¹: {r['content']}\næ¥æº: {r['url']}" for r in results]
    return {"documents": web_docs}


def hallucination_grader(state):
    llm = get_fast_llm()
    res = (ChatPromptTemplate.from_messages([("system", "åˆ¤æ–­å›ç­”æ˜¯å¦åŸºäºå‚è€ƒèµ„æ–™ã€‚"), ("human", "èµ„æ–™: {documents} \nå›ç­”: {generation}")]) | llm.with_structured_output(
        grade_schema)).invoke({"documents": ' '.join(state["documents"]), "generation": state["generation"]})
    return {"hallucination_score": res["score"] if isinstance(res, dict) else res.score}


def answer_grader(state):
    llm = get_fast_llm()
    res = (ChatPromptTemplate.from_messages([("system", "åˆ¤æ–­å›ç­”æ˜¯å¦è§£å†³äº†ç”¨æˆ·é—®é¢˜ã€‚"), ("human", "é—®é¢˜: {question} \nå›ç­”: {generation}")]) | llm.with_structured_output(
        grade_schema)).invoke({"question": state["question"], "generation": state["generation"]})
    return {"answer_score": res["score"] if isinstance(res, dict) else res.score}

# --- 5. æ„å»ºå·¥ä½œæµ ---


def dec_gen(
    state): return "web_search" if state["web_search"] == "Yes" else "generate"


def dec_final(state):
    if state.get("retry_count", 0) >= 2:
        return "useful"
    if state["hallucination_score"] == "no":
        return "not supported"
    return "useful" if state["answer_score"] == "yes" else "not useful"


workflow = StateGraph(GraphState)
workflow.add_node("contextualize_question", contextualize_question)
workflow.add_node("route_question", route_question)
workflow.add_node("handle_off_topic", handle_off_topic)
workflow.add_node("sql_agent", call_sql_agent)
workflow.add_node("retrieve", retrieve)
workflow.add_node("grade_documents", grade_documents)
workflow.add_node("generate", generate)
workflow.add_node("web_search", web_search)
workflow.add_node("hallucination_grader", hallucination_grader)
workflow.add_node("answer_grader", answer_grader)

workflow.add_edge(START, "contextualize_question")
workflow.add_edge("contextualize_question", "route_question")
workflow.add_conditional_edges("route_question", lambda x: x["router_decision"], {
                               "sql_db": "sql_agent", "vector_db": "retrieve", "off_topic": "handle_off_topic"})
workflow.add_edge("handle_off_topic", END)
workflow.add_edge("sql_agent", END)
workflow.add_edge("retrieve", "grade_documents")
workflow.add_conditional_edges("grade_documents", dec_gen, {
                               "web_search": "web_search", "generate": "generate"})
workflow.add_edge("web_search", "generate")
workflow.add_edge("generate", "hallucination_grader")
workflow.add_edge("hallucination_grader", "answer_grader")
workflow.add_conditional_edges("answer_grader", dec_final, {
                               "useful": END, "not useful": "web_search", "not supported": "generate"})

app = workflow.compile()

# --- 6. ç»Ÿä¸€æŸ¥è¯¢å…¥å£ ---


def query_with_graph(question: str, image_bytes: bytes = None):
    start_time = time.time()
    target_lang = st.session_state.get("target_language", "è‡ªåŠ¨è¯†åˆ« (Auto)")

    # 1. ç¡®ä¿ç¼“å­˜å·²åˆå§‹åŒ–
    if "response_cache" not in st.session_state:
        st.session_state.response_cache = {}

    # 2. å›¾ç‰‡è¯†åˆ«é€»è¾‘ (ç‹¬ç«‹è¿è¡Œ)
    if image_bytes:
        from agent_logic import query_text as vision_query
        res, dur = vision_query(question, image_bytes=image_bytes)
        yield {"node": "end", "generation": res, "duration": dur}
        return

    # 3. ç”Ÿæˆè¯­ä¹‰æŒ‡çº¹ä½œä¸ºç¼“å­˜ Key (ç»Ÿä¸€å½’ä¸€åŒ–ä¸ºè‹±æ–‡)
    # è¿™ä¸€æ­¥æ˜¯å…³é”®ï¼šè®©â€œèƒ½å–å—â€å’Œâ€œCan I drinkâ€åœ¨åå°éƒ½ç”Ÿæˆç›¸åŒçš„è‹±æ–‡å¥å­
    fingerprint_res = contextualize_question(
        {"question": question, "target_language": "Normalized_Key"})
    semantic_key = fingerprint_res.get("question", "").lower().strip()

    # --- æ‰“å°è§‚å¯Ÿç»“æœ ---
    print(f"\n{'='*20} [SEMANTIC CACHE] {'='*20}")
    print(f"ã€æ•è·æŒ‡çº¹ã€‘: {semantic_key}")
    # é¢„æœŸè¾“å‡ºï¼šallergycheck|lee kum kee|dark soy sauce
    print(f"{'='*55}\n")

    display_q_res = contextualize_question(
        {"question": question, "target_language": target_lang})
    refined_q = display_q_res["question"]
    # --- æ‰“å° Key ä¾›ä½ è§‚å¯Ÿ ---
    print(f"ã€æ ‡å‡†åŒ–è¯­ä¹‰ Keyã€‘: {semantic_key}")

    # 4. ç”Ÿæˆå±•ç¤ºç”¨çš„è¡¥å…¨æ„å›¾ (ç”¨äº UI æ˜¾ç¤º)
    display_q_res = contextualize_question(
        {"question": question, "target_language": target_lang})
    refined_q = display_q_res.get("question", question)
    yield {"node": "contextualize_question", "status": "complete", "refined_q": refined_q}

    # 5. è¯­ä¹‰çº§ç¼“å­˜æ£€æŸ¥
    if semantic_key in st.session_state.response_cache:
        yield {"node": "cache_hit", "status": "complete"}
        final_res = st.session_state.response_cache[semantic_key]
    else:
        # 6. ç¼“å­˜æœªå‘½ä¸­ï¼šæ‰§è¡Œæ­£å¼å·¥ä½œæµ
        final_res = "æŠ±æ­‰ï¼Œç”±äºé€»è¾‘å¼‚å¸¸ã€‚"
        # ä¼ å…¥è¡¥å…¨åçš„é—®é¢˜ï¼Œå¹¶è·³è¿‡å·¥ä½œæµå†…éƒ¨çš„é‡å¤è¡¥å…¨èŠ‚ç‚¹
        for event in app.stream({"question": refined_q, "target_language": target_lang, "retry_count": 0}, stream_mode="updates"):
            for node_name, output in event.items():
                if node_name == "contextualize_question":
                    continue
                yield {"node": node_name, "status": "running"}
                if "generation" in output:
                    final_res = output["generation"]

        # å°†ç»“æœå­˜å…¥ç¼“å­˜ (ä½¿ç”¨è¯­ä¹‰æŒ‡çº¹)
        st.session_state.response_cache[semantic_key] = final_res

    yield {"node": "end", "generation": final_res, "duration": time.time() - start_time}
