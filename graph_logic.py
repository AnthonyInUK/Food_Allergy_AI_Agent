import os
import time
import base64
import hashlib
import asyncio
from typing import List, TypedDict, Annotated, Union
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
import streamlit as st
from langgraph.graph import END, StateGraph, START

from agent_logic import get_sql_agent, get_db, get_llm, query_text as sql_query_text

load_dotenv()

# --- 1. èµ„æºç¼“å­˜æ±  ---


@st.cache_resource
def get_fast_llm():
    return ChatOpenAI(model="gpt-4o-mini", temperature=0)


@st.cache_resource
def get_vectorstore():
    import os
    try:
        embeddings = OpenAIEmbeddings()
        persist_dir = "data/chroma_db"
        # Ensure directory exists
        os.makedirs(persist_dir, exist_ok=True)
        return Chroma(persist_directory=persist_dir, embedding_function=embeddings)
    except Exception as e:
        print(f"Warning: Failed to initialize vectorstore: {e}")
        # Return None and handle gracefully in query logic
        return None


# --- ç¼“å­˜è¾…åŠ©å‡½æ•° ---
def get_semantic_hash(text: str) -> str:
    """å°†è¯­ä¹‰é”®è½¬æ¢ä¸º MD5 å“ˆå¸Œï¼Œæé«˜ç¼“å­˜é”®æ•ˆç‡"""
    return hashlib.md5(text.encode('utf-8')).hexdigest()


def init_cache_system():
    """åˆå§‹åŒ–å¤šå±‚ç¼“å­˜ç³»ç»Ÿ"""
    if "response_cache" not in st.session_state:
        st.session_state.response_cache = {}  # L1: æœ€ç»ˆç­”æ¡ˆç¼“å­˜ï¼ˆè¯­ä¹‰çº§ï¼Œè·¨è¯­è¨€ï¼‰
    if "retrieval_cache" not in st.session_state:
        st.session_state.retrieval_cache = {}  # L2: å‘é‡æ£€ç´¢ç»“æœç¼“å­˜
    if "generation_cache" not in st.session_state:
        st.session_state.generation_cache = {}  # L3: LLM ç”Ÿæˆç»“æœç¼“å­˜
    if "grade_cache" not in st.session_state:
        st.session_state.grade_cache = {}  # L4: æ–‡æ¡£è¯„ä¼°ç¼“å­˜
    if "hallucination_cache" not in st.session_state:
        st.session_state.hallucination_cache = {}  # L5: å¹»è§‰æ£€æµ‹ç¼“å­˜
    if "answer_grade_cache" not in st.session_state:
        st.session_state.answer_grade_cache = {}  # L6: ç­”æ¡ˆè´¨é‡è¯„ä¼°ç¼“å­˜
    if "cache_stats" not in st.session_state:
        st.session_state.cache_stats = {"hits": 0, "misses": 0}  # ç¼“å­˜ç»Ÿè®¡


def get_cache_stats():
    """è·å–ç¼“å­˜å‘½ä¸­ç‡ç»Ÿè®¡"""
    stats = st.session_state.get("cache_stats", {"hits": 0, "misses": 0})
    total = stats["hits"] + stats["misses"]
    hit_rate = (stats["hits"] / total * 100) if total > 0 else 0
    return {"hit_rate": hit_rate, "total_queries": total, **stats}


def clear_all_caches():
    """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜ï¼ˆç”¨äºè°ƒè¯•æˆ–é‡Šæ”¾å†…å­˜ï¼‰"""
    cache_types = [
        "response_cache",
        "retrieval_cache",
        "generation_cache",
        "grade_cache",
        "hallucination_cache",
        "answer_grade_cache"
    ]

    cleared_count = 0
    for cache_name in cache_types:
        if cache_name in st.session_state:
            st.session_state[cache_name].clear()
            cleared_count += 1

    if "cache_stats" in st.session_state:
        st.session_state.cache_stats = {"hits": 0, "misses": 0}

    print(f"âœ“ å·²æ¸…ç©º {cleared_count} ä¸ªç¼“å­˜å±‚")


# --- 2. ç»“æ„åŒ–è¾“å‡º Schema ---
route_schema = {
    "name": "route_query",
    "description": "åˆ¤å®šç”¨æˆ·æŸ¥è¯¢çš„æ„å›¾å’Œé¢†åŸŸèŒƒå›´",
    "parameters": {
        "type": "object",
        "properties": {
            "datasource": {"type": "string", "enum": ["sql_db", "vector_db", "off_topic"]}
        },
        "required": ["datasource"]
    }
}

grade_schema = {
    "name": "grade",
    "parameters": {
        "type": "object",
        "properties": {"score": {"type": "string", "enum": ["yes", "no"]}},
        "required": ["score"]
    }
}

# --- 3. çŠ¶æ€å®šä¹‰ ---


class GraphState(TypedDict):
    question: str
    generation: str
    web_search: str
    documents: List[str]
    router_decision: str
    hallucination_score: str
    answer_score: str
    retry_count: int
    target_language: str

# --- 4. èŠ‚ç‚¹é€»è¾‘ ---


def contextualize_question(state):
    print("--- ğŸš¦ æ­£åœ¨è¿›è¡Œä¸Šä¸‹æ–‡è¡¥å…¨ ---")
    question = state["question"]
    target_lang = state.get("target_language", "è‡ªåŠ¨è¯†åˆ« (Auto)")

    msgs = StreamlitChatMessageHistory(key="messages")
    history = msgs.messages[:-1][-5:] if len(msgs.messages) > 1 else []

    # ğŸ”§ ä¿®å¤ï¼šNormalized_Key æ¨¡å¼ä¸‹å³ä½¿æ²¡æœ‰å†å²ä¹Ÿè¦è½¬æ¢
    if not history and target_lang != "Normalized_Key":
        return {"question": question}

    llm = get_fast_llm()
    system_instruction = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®é¢˜é‡å†™ä¸“å®¶ã€‚
        ä½ çš„ä»»åŠ¡æ˜¯ï¼šæ ¹æ®å¯¹è¯å†å²ï¼Œå°†ç”¨æˆ·æœ€æ–°çš„æé—®æ”¹å†™ä¸ºä¸€ä¸ªã€å®Œå…¨ç‹¬ç«‹ã€æ— æ­§ä¹‰ã€‘çš„é—®é¢˜ã€‚
        
        ã€æ ¸å¿ƒè¦æ±‚ã€‘
        1. æ¶ˆé™¤ä»£è¯ï¼šå¿…é¡»å°†"è¿™ä¸ª"ã€"å®ƒ"ã€"this"ã€"it"ç­‰è¯ï¼Œæ›¿æ¢ä¸ºå†å²å¯¹è¯ä¸­æåˆ°çš„å…·ä½“é£Ÿå“åç§°æˆ–å“ç‰Œã€‚
        2. è·¨è¯­è¨€å¯¹é½ï¼šå³ä½¿å†å²æ˜¯ä¸­æ–‡è€Œå½“å‰æé—®æ˜¯è‹±æ–‡ï¼ˆæˆ–åä¹‹ï¼‰ï¼Œä½ ä¹Ÿå¿…é¡»å‡†ç¡®æå–å“ç‰Œåï¼ˆå¦‚æé”¦è®°/Lee Kum Keeï¼‰å¹¶åµŒå…¥æ–°é—®é¢˜ä¸­ã€‚
        3. ä¸¥ç¦å·æ‡’ï¼šä¸¥ç¦è¾“å‡ºç±»ä¼¼ "this product" æˆ– "the sauce" è¿™ç§ä¾ç„¶æ¨¡ç³Šçš„è¯ï¼Œå¿…é¡»è¯´å‡ºå…¨åã€‚
        4. ä¿æŒåŸæ„ï¼šä¸è¦å›ç­”é—®é¢˜ï¼Œåªéœ€é‡å†™æé—®ã€‚ç›´æ¥è¾“å‡ºé‡å†™åçš„ç»“æœã€‚
        """

    # æ ¸å¿ƒä¼˜åŒ–ç‚¹ï¼šå¦‚æœæˆ‘ä»¬è¦ç”Ÿæˆ Keyï¼Œä½¿ç”¨ä¸€ç§æå…¶æ­»æ¿çš„æ ¼å¼
    if target_lang == "Normalized_Key":
        system_instruction = """ä½ æ˜¯ä¸€ä¸ªå¤šè¯­è¨€å®ä½“å¯¹é½ä¸“å®¶ã€‚
        ä½ çš„ä»»åŠ¡ï¼šæå–é—®é¢˜ä¸­çš„æ ¸å¿ƒæ„å›¾ï¼Œå¹¶ã€å¼ºåˆ¶ç»Ÿä¸€ç¿»è¯‘ä¸ºè‹±æ–‡æ ‡å‡†æ ¼å¼ã€‘ã€‚
        
        å¿…é¡»è¾“å‡ºæ­¤æ ¼å¼ï¼š[æ„å›¾]|[è‹±æ–‡å“ç‰Œ]|[è‹±æ–‡äº§å“å]
        æ„å›¾åˆ†ç±»ï¼šAllergyCheck, InfoSearch, Appearance, Compare, List
        ç¿»è¯‘ç¤ºä¾‹ï¼š
        - "æé”¦è®°" -> "Lee Kum Kee"
        - "è€æŠ½" -> "Dark Soy Sauce"
        - "èƒ½åƒå—" / "can i eat" -> "AllergyCheck"
        - "é•¿ä»€ä¹ˆæ ·" / "look like" -> "Appearance"
        - "å¯¹æ¯”" / "compare" -> "Compare"
        
        è¾“å‡ºç¤ºä¾‹ï¼š
        - "æˆ‘å¯¹å¤§è±†è¿‡æ•ï¼Œèƒ½å–æé”¦è®°è€æŠ½å—" -> AllergyCheck|Lee Kum Kee|Dark Soy Sauce
        - "I'm allergic to soy. Can I have Lee Kum Kee dark soy sauce?" -> AllergyCheck|Lee Kum Kee|Dark Soy Sauce
        
        âš ï¸ ä¸¥ç¦è¾“å‡ºä»»ä½•ä¸­æ–‡æˆ–å¤šä½™å•è¯ï¼å¿…é¡»å®Œå…¨æŒ‰ç…§æ ¼å¼è¾“å‡ºï¼"""
    else:
        # ç”¨äº UI å±•ç¤ºçš„æç¤ºè¯ä¿æŒåŸæœ‰çš„çµæ´»æ€§
        system_instruction = "ä½ æ˜¯ä¸€ä¸ªé—®é¢˜é‡å†™ä¸“å®¶ã€‚æ ¹æ®å¯¹è¯å†å²ï¼Œå°†æé—®æ”¹å†™ä¸ºç‹¬ç«‹çš„å®Œæ•´æé—®ã€‚å¤„ç†ä»£è¯æŒ‡ä»£ã€‚"

    if target_lang == "English":
        system_instruction += " è¯·åŠ¡å¿…ä½¿ç”¨ã€è‹±æ–‡ã€‘é‡å†™é—®é¢˜ã€‚"
    elif target_lang == "FranÃ§ais":
        system_instruction += " è¯·åŠ¡å¿…ä½¿ç”¨ã€æ³•è¯­ã€‘é‡å†™é—®é¢˜ã€‚"
    elif target_lang == "è‡ªåŠ¨è¯†åˆ« (Auto)":
        system_instruction += " è¯·ä¿æŒä¸ç”¨æˆ·æé—®ç›¸åŒçš„è¯­è¨€ã€‚"
    else:
        system_instruction += " è¯·ä½¿ç”¨ã€ä¸­æ–‡ã€‘é‡å†™é—®é¢˜ã€‚"

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_instruction),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{question}")
    ])
    res = (prompt | llm).invoke({"history": history, "question": question})
    print(f"ã€é—®é¢˜è¡¥å…¨ç»“æœã€‘: {res.content}")
    return {"question": res.content}


def route_question(state):
    """ã€100% è¿˜åŸä½ æœ€æ»¡æ„çš„é«˜ç²¾åº¦è·¯ç”±æç¤ºè¯ã€‘"""
    print("--- æ™ºèƒ½è·¯ç”±ä¸å®‰å…¨ç½‘å…³ ---")
    llm = get_fast_llm()
    structured_llm = llm.with_structured_output(route_schema)

    system = """ä½ æ˜¯ä¸€ä¸ªé«˜ç²¾åº¦çš„é£Ÿå“å®‰å…¨è·¯ç”±ä¸“å®¶ã€‚
    ã€æ ¸å¿ƒåˆ†å‘è§„åˆ™ - å¿…é¡»æ­»å®ˆã€‘
    1. å¿…é¡»é€‰ 'sql_db' (ç²¾å‡†æŸ¥è¯¢)ï¼š
       - é—®é¢˜ä¸­åŒ…å«ã€å…·ä½“å“ç‰Œã€‘ï¼ˆå¦‚ï¼šæé”¦è®°ã€Lee Kum Keeã€åº·å¸ˆå‚…ã€æµ·å¤©ç­‰ï¼‰ï¼Œå¿…é¡»é€‰æ­¤é¡¹ã€‚
       - å³ä¾¿æ˜¯é—®â€œèƒ½åƒå—â€ã€â€œå«å¤§è±†å—â€ï¼Œåªè¦æœ‰å“ç‰Œï¼Œä¹Ÿå¿…é¡»è·¯ç”±åˆ° 'sql_db'ã€‚
       - é—®é¢˜æ¶‰åŠã€ç‰¹å®šäº§å“åã€‘ï¼ˆå¦‚ï¼šè€æŠ½ã€é…±æ²¹ã€ç•ªèŒ„é…±ï¼‰ã€‚
       - é—®é¢˜è¦æ±‚çœ‹ã€å›¾ç‰‡/å¤–è§‚/é•¿ä»€ä¹ˆæ ·/åŒ…è£…ã€‘ã€‚
       - æ¶‰åŠç»Ÿè®¡æˆ–åˆ—è¡¨ï¼ˆå¦‚ï¼šæœ‰å“ªäº›ä¸å«å¤§è±†çš„é…±ï¼Ÿï¼‰ã€‚
    
    2. å¿…é¡»é€‰ 'vector_db' (å¸¸è¯†/å»ºè®®)ï¼š
    
     - ä»…å½“é—®é¢˜æ˜¯ã€ä¸€èˆ¬æ€§çŸ¥è¯†ã€‘ï¼ˆå¦‚ï¼šä»€ä¹ˆæ˜¯é¢ç­‹ï¼Ÿï¼‰æˆ–ã€å®Œå…¨æ²¡æœ‰å“ç‰Œåã€‘çš„æ¨¡ç³Šå’¨è¯¢ã€‚
       - å…³äºã€æˆåˆ†çŸ¥è¯†ã€‘ï¼ˆå¦‚ï¼šä»€ä¹ˆæ˜¯é¢ç­‹ï¼Ÿé˜²è…å‰‚æœ‰å®³å—ï¼Ÿï¼‰ã€‚
       - åªè¦æ¶‰åŠé£Ÿå“ã€è¿‡æ•ã€èº«ä½“å®‰å…¨ï¼Œå¿…é¡»åœ¨è¿™ 1 å’Œ 2 ä¹‹é—´é€‰æ‹©ï¼
    
    3. å¿…é¡»é€‰ 'off_topic' (ä¸¥æ ¼æ‹¦æˆª)ï¼š
       - ä»…å½“é—®é¢˜ä¸ã€é£Ÿå“ã€è¿‡æ•ã€å“ç‰Œã€è¥å…»ã€å®‰å…¨ã€‘å®Œå…¨æ— å…³æ—¶ã€‚
       - ä¾‹å¦‚ï¼šé—®ä»£ç ã€æ”¿æ²»ã€é—²èŠã€é—®å¤©æ°”ã€é—®æ•°å­¦é¢˜ç­‰ã€‚
    """
    # æˆ‘ä»¬æŠŠè¿™ä¸ªä½œä¸ºâ€œç¡¬æç¤ºâ€å¡ç»™è·¯ç”±å™¨
    question_to_route = state["question"]
    # æå–å“ç‰Œåå•ï¼ˆå¯ä»¥åŠ¨æ€è·å–ï¼Œè¿™é‡Œç¤ºä¾‹å‡ ä¸ªæ ¸å¿ƒå“ç‰Œï¼‰
    known_brands = ["lee kum kee", "æé”¦è®°", "haday", "æµ·å¤©", "master kong", "åº·å¸ˆå‚…"]

    # å¦‚æœé—®é¢˜é‡Œæœ‰è¿™äº›è¯ï¼Œæˆ‘ä»¬åœ¨é—®é¢˜æœ«å°¾å¼ºè¡ŒåŠ ä¸ªæç¤º
    if any(brand in question_to_route.lower() for brand in known_brands):
        question_to_route += " (Note: This question contains a specific brand, prioritize structured data source.)"

    res = (ChatPromptTemplate.from_messages(
        [("system", system), ("human", "{question}")]) | structured_llm).invoke({"question": state["question"]})
    decision = res["datasource"] if isinstance(res, dict) else res.datasource
    return {"router_decision": decision}


def handle_off_topic(state):
    print("--- ğŸš« æ‹¦æˆªéæ³•è¯·æ±‚ ---")
    target_lang = state.get("target_language", "è‡ªåŠ¨è¯†åˆ« (Auto)")
    msg = "æŠ±æ­‰ï¼Œæˆ‘æ˜¯ä¸€åä¸“ä¸šçš„é£Ÿå“è¿‡æ•åŠ©æ‰‹ã€‚æˆ‘åªèƒ½å›ç­”ä¸é£Ÿå“æˆåˆ†ã€è¿‡æ•åŸã€é£Ÿå“å“ç‰Œä»¥åŠé£Ÿå“å®‰å…¨ç›¸å…³çš„é—®é¢˜ã€‚"
    if target_lang == "English":
        msg = "Sorry, I am a professional Food Allergy Assistant. I can only answer questions related to food ingredients, allergens, brands, and safety."
    elif target_lang == "FranÃ§ais":
        msg = "DÃ©solÃ©, je suis un assistant professionnel pour les allergies alimentaires. Je ne peux rÃ©pondre qu'aux questions relatives aux ingrÃ©dients, aux allergÃ¨nes, aux marques et Ã  la sÃ©curitÃ© alimentaire."
    return {"generation": msg}


def call_sql_agent(state):
    print("--- å¯åŠ¨ SQL Agent ---")
    response, _ = sql_query_text(state["question"])
    return {"generation": response}


def retrieve(state):
    print("--- æ£€ç´¢æœ¬åœ°çŸ¥è¯†åº“ ---")
    question = state["question"]

    # æ£€ç´¢ç¼“å­˜ï¼šå¯¹ç›¸åŒé—®é¢˜çš„å‘é‡æ£€ç´¢ç»“æœè¿›è¡Œç¼“å­˜
    if "retrieval_cache" in st.session_state:
        cache_key = get_semantic_hash(question.lower().strip())
        if cache_key in st.session_state.retrieval_cache:
            print("  âœ“ å‘½ä¸­æ£€ç´¢ç¼“å­˜")
            return {"documents": st.session_state.retrieval_cache[cache_key]}

    docs = get_vectorstore().similarity_search(question, k=3)
    doc_texts = [
        f"å†…å®¹: {d.page_content}\næ¥æº: {d.metadata.get('source', 'æœ¬åœ°çŸ¥è¯†åº“')}" for d in docs]

    # å­˜å…¥æ£€ç´¢ç¼“å­˜
    if "retrieval_cache" in st.session_state:
        st.session_state.retrieval_cache[cache_key] = doc_texts

    return {"documents": doc_texts}


def grade_documents(state):
    """è¯„ä¼°æ–‡æ¡£è´¨é‡ï¼ˆä¼˜åŒ–ç‰ˆï¼šå¯¹è¯„ä¼°ç»“æœè¿›è¡Œç¼“å­˜ï¼‰"""
    if not state.get("documents"):
        return {"web_search": "Yes"}

    # åˆå§‹åŒ–è¯„ä¼°ç¼“å­˜
    if "grade_cache" not in st.session_state:
        st.session_state.grade_cache = {}

    # ç”Ÿæˆç¼“å­˜é”®ï¼šé—®é¢˜ + æ–‡æ¡£å†…å®¹
    question = state["question"]
    docs_text = ' '.join(state["documents"])
    cache_key = get_semantic_hash(f"grade|{question}|{docs_text}")

    # æ£€æŸ¥ç¼“å­˜
    if cache_key in st.session_state.grade_cache:
        print("  âœ“ å‘½ä¸­æ–‡æ¡£è¯„ä¼°ç¼“å­˜")
        return {"web_search": st.session_state.grade_cache[cache_key]}

    # ç¼“å­˜æœªå‘½ä¸­ï¼šæ‰§è¡Œ LLM è¯„ä¼°
    llm = get_fast_llm()
    res = (ChatPromptTemplate.from_messages([("system", "åˆ¤æ–­èµ„æ–™æ˜¯å¦è¶³ä»¥å›ç­”é—®é¢˜ã€‚"), ("human", "é—®é¢˜: {question} \nèµ„æ–™: {documents}")]) | llm.with_structured_output(
        grade_schema)).invoke({"question": question, "documents": docs_text})
    score = res["score"] if isinstance(res, dict) else res.score
    result = "No" if score == "yes" else "Yes"

    # å­˜å…¥ç¼“å­˜
    st.session_state.grade_cache[cache_key] = result

    return {"web_search": result}


def generate(state):
    print("--- ç”Ÿæˆå›ç­” ---")
    retry_count = state.get("retry_count", 0) + 1
    target_lang = state.get("target_language", "è‡ªåŠ¨è¯†åˆ« (Auto)")

    # ç”Ÿæˆç¼“å­˜é”®ï¼šåŸºäºé—®é¢˜ + æ–‡æ¡£å†…å®¹ + è¯­è¨€
    if "generation_cache" not in st.session_state:
        st.session_state.generation_cache = {}

    docs_text = ' '.join(state.get("documents", []))
    cache_key = get_semantic_hash(
        f"{state['question']}|{docs_text}|{target_lang}")

    # æ£€æŸ¥ç”Ÿæˆç¼“å­˜
    if cache_key in st.session_state.generation_cache:
        print("  âœ“ å‘½ä¸­ç”Ÿæˆç¼“å­˜ï¼ˆè·³è¿‡ LLM è°ƒç”¨ï¼‰")
        return {"generation": st.session_state.generation_cache[cache_key], "retry_count": retry_count}

    lang_instruction = ""
    if target_lang == "ç®€ä½“ä¸­æ–‡":
        lang_instruction = "è¯·ä½¿ç”¨ã€ç®€ä½“ä¸­æ–‡ã€‘å›ç­”ã€‚"
    elif target_lang == "English":
        lang_instruction = "Please reply in ã€Englishã€‘."
    elif target_lang == "FranÃ§ais":
        lang_instruction = "RÃ©pondez en ã€FranÃ§aisã€‘."
    else:
        lang_instruction = "è¯·ä½¿ç”¨ç”¨æˆ·æé—®çš„è¯­è¨€å›ç­”ã€‚"

    llm = get_llm()
    prompt = ChatPromptTemplate.from_template(
        f"ä½ æ˜¯ä¸€ä¸ªé£Ÿå“è¿‡æ•ä¸“å®¶ã€‚\n{lang_instruction}\nã€é‡è¦ã€‘å¿…é¡»åœ¨æœ«å°¾åˆ—å‡ºå‚è€ƒæ¥æºã€‚\nèµ„æ–™: {{documents}}\né—®é¢˜: {{question}}")
    response = (prompt | llm).invoke(
        {"documents": state["documents"], "question": state["question"]})

    # å­˜å…¥ç”Ÿæˆç¼“å­˜
    st.session_state.generation_cache[cache_key] = response.content

    return {"generation": response.content, "retry_count": retry_count}


def web_search(state):
    print("--- è§¦å‘è”ç½‘æœç´¢ ---")
    from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper
    search = TavilySearchResults(api_wrapper=TavilySearchAPIWrapper(
        tavily_api_key=os.getenv("TAVILY_API_KEY")), k=3)
    results = search.invoke({"query": state["question"]})
    web_docs = [f"å†…å®¹: {r['content']}\næ¥æº: {r['url']}" for r in results]
    return {"documents": web_docs}


def hallucination_grader(state):
    """å¹»è§‰æ£€æµ‹ï¼ˆä¼˜åŒ–ç‰ˆï¼šå¯¹æ£€æµ‹ç»“æœè¿›è¡Œç¼“å­˜ï¼‰"""
    # åˆå§‹åŒ–å¹»è§‰æ£€æµ‹ç¼“å­˜
    if "hallucination_cache" not in st.session_state:
        st.session_state.hallucination_cache = {}

    # ç”Ÿæˆç¼“å­˜é”®ï¼šæ–‡æ¡£ + å›ç­”
    docs_text = ' '.join(state["documents"])
    generation = state["generation"]
    cache_key = get_semantic_hash(f"hallucination|{docs_text}|{generation}")

    # æ£€æŸ¥ç¼“å­˜
    if cache_key in st.session_state.hallucination_cache:
        print("  âœ“ å‘½ä¸­å¹»è§‰æ£€æµ‹ç¼“å­˜")
        return {"hallucination_score": st.session_state.hallucination_cache[cache_key]}

    # ç¼“å­˜æœªå‘½ä¸­ï¼šæ‰§è¡Œ LLM åˆ¤æ–­
    llm = get_fast_llm()
    res = (ChatPromptTemplate.from_messages([("system", "åˆ¤æ–­å›ç­”æ˜¯å¦åŸºäºå‚è€ƒèµ„æ–™ã€‚"), ("human", "èµ„æ–™: {documents} \nå›ç­”: {generation}")]) | llm.with_structured_output(
        grade_schema)).invoke({"documents": docs_text, "generation": generation})
    score = res["score"] if isinstance(res, dict) else res.score

    # å­˜å…¥ç¼“å­˜
    st.session_state.hallucination_cache[cache_key] = score

    return {"hallucination_score": score}


def answer_grader(state):
    """ç­”æ¡ˆè´¨é‡è¯„ä¼°ï¼ˆä¼˜åŒ–ç‰ˆï¼šå¯¹è¯„ä¼°ç»“æœè¿›è¡Œç¼“å­˜ï¼‰"""
    # åˆå§‹åŒ–ç­”æ¡ˆè¯„ä¼°ç¼“å­˜
    if "answer_grade_cache" not in st.session_state:
        st.session_state.answer_grade_cache = {}

    # ç”Ÿæˆç¼“å­˜é”®ï¼šé—®é¢˜ + å›ç­”
    question = state["question"]
    generation = state["generation"]
    cache_key = get_semantic_hash(f"answer|{question}|{generation}")

    # æ£€æŸ¥ç¼“å­˜
    if cache_key in st.session_state.answer_grade_cache:
        print("  âœ“ å‘½ä¸­ç­”æ¡ˆè¯„ä¼°ç¼“å­˜")
        return {"answer_score": st.session_state.answer_grade_cache[cache_key]}

    # ç¼“å­˜æœªå‘½ä¸­ï¼šæ‰§è¡Œ LLM åˆ¤æ–­
    llm = get_fast_llm()
    res = (ChatPromptTemplate.from_messages([("system", "åˆ¤æ–­å›ç­”æ˜¯å¦è§£å†³äº†ç”¨æˆ·é—®é¢˜ã€‚"), ("human", "é—®é¢˜: {question} \nå›ç­”: {generation}")]) | llm.with_structured_output(
        grade_schema)).invoke({"question": question, "generation": generation})
    score = res["score"] if isinstance(res, dict) else res.score

    # å­˜å…¥ç¼“å­˜
    st.session_state.answer_grade_cache[cache_key] = score

    return {"answer_score": score}


def parallel_graders(state):
    """ğŸš€ å¹¶è¡Œæ‰§è¡Œå¹»è§‰æ£€æµ‹å’Œç­”æ¡ˆè¯„ä¼°ï¼ˆèŠ‚çœ 40-50% æ—¶é—´ï¼‰"""
    print("--- ğŸš€ å¹¶è¡Œæ‰§è¡Œè´¨é‡è¯„ä¼° ---")

    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œä¸¤ä¸ªç‹¬ç«‹çš„è¯„ä¼°
    with ThreadPoolExecutor(max_workers=2) as executor:
        # åŒæ—¶æäº¤ä¸¤ä¸ªä»»åŠ¡
        future_hallucination = executor.submit(hallucination_grader, state)
        future_answer = executor.submit(answer_grader, state)

        # ç­‰å¾…ä¸¤ä¸ªä»»åŠ¡å®Œæˆ
        hallucination_result = future_hallucination.result()
        answer_result = future_answer.result()

    print("  âœ… å¹¶è¡Œè¯„ä¼°å®Œæˆ")

    # åˆå¹¶ç»“æœ
    return {
        "hallucination_score": hallucination_result["hallucination_score"],
        "answer_score": answer_result["answer_score"]
    }

# --- 5. æ„å»ºå·¥ä½œæµ ---


def dec_gen(
    state): return "web_search" if state["web_search"] == "Yes" else "generate"


def dec_final(state):
    if state.get("retry_count", 0) >= 2:
        return "useful"
    if state["hallucination_score"] == "no":
        return "not supported"
    return "useful" if state["answer_score"] == "yes" else "not useful"


workflow = StateGraph(GraphState)
workflow.add_node("contextualize_question", contextualize_question)
workflow.add_node("route_question", route_question)
workflow.add_node("handle_off_topic", handle_off_topic)
workflow.add_node("sql_agent", call_sql_agent)
workflow.add_node("retrieve", retrieve)
workflow.add_node("grade_documents", grade_documents)
workflow.add_node("generate", generate)
workflow.add_node("web_search", web_search)
workflow.add_node("parallel_graders", parallel_graders)  # ğŸš€ æ–°å¢å¹¶è¡Œè¯„ä¼°èŠ‚ç‚¹

workflow.add_edge(START, "contextualize_question")
workflow.add_edge("contextualize_question", "route_question")
workflow.add_conditional_edges("route_question", lambda x: x["router_decision"], {
                               "sql_db": "sql_agent", "vector_db": "retrieve", "off_topic": "handle_off_topic"})
workflow.add_edge("handle_off_topic", END)
workflow.add_edge("sql_agent", END)
workflow.add_edge("retrieve", "grade_documents")
workflow.add_conditional_edges("grade_documents", dec_gen, {
                               "web_search": "web_search", "generate": "generate"})
workflow.add_edge("web_search", "generate")
workflow.add_edge("generate", "parallel_graders")  # ğŸš€ æ”¹ä¸ºå¹¶è¡Œè¯„ä¼°
workflow.add_conditional_edges("parallel_graders", dec_final, {  # ğŸš€ ä»å¹¶è¡ŒèŠ‚ç‚¹å†³ç­–
                               "useful": END, "not useful": "web_search", "not supported": "generate"})

app = workflow.compile()

# --- 6. ç»Ÿä¸€æŸ¥è¯¢å…¥å£ ---


def query_with_graph(question: str, image_bytes: bytes = None):
    start_time = time.time()
    target_lang = st.session_state.get("target_language", "è‡ªåŠ¨è¯†åˆ« (Auto)")

    # 1. åˆå§‹åŒ–å¤šå±‚ç¼“å­˜ç³»ç»Ÿ
    init_cache_system()

    # 2. å›¾ç‰‡è¯†åˆ«é€»è¾‘ (ç‹¬ç«‹è¿è¡Œï¼Œä¸ä½¿ç”¨ç¼“å­˜)
    if image_bytes:
        from agent_logic import query_text as vision_query
        res, dur = vision_query(question, image_bytes=image_bytes)
        yield {"node": "end", "generation": res, "duration": dur}
        return

    # ğŸš€ 3. è¶…å¿«é€Ÿè·¯å¾„æ£€æµ‹ï¼šåœ¨å·¥ä½œæµä¹‹å‰å°±æ‹¦æˆªç®€å•æŸ¥è¯¢
    from agent_logic import query_text as direct_sql_query_func

    # æ£€æµ‹å¸¸è§å“ç‰Œ + å›¾ç‰‡/è¿‡æ•åŸå…³é”®è¯
    q_lower = question.lower()
    quick_brands = ["æé”¦è®°", "lee kum kee", "æµ·å¤©", "haday", "åº·å¸ˆå‚…", "master kong"]
    quick_keywords = ["é•¿ä»€ä¹ˆæ ·", "çœ‹å›¾", "å›¾ç‰‡", "å¤–è§‚", "åŒ…è£…", "èƒ½åƒ", "è¿‡æ•",
                      "look like", "picture", "image", "allerg", "safe"]

    # æ’é™¤å…³é”®è¯ï¼šè¿™äº›é—®é¢˜éœ€è¦ Agent å¤šè·³æ¨ç†
    # ä½¿ç”¨å•è¯è¾¹ç•Œé¿å…è¯¯åŒ¹é…ï¼ˆå¦‚ "all" ä¸åº”åŒ¹é… "allergic"ï¼‰
    exclude_keywords = ["å¯¹æ¯”", "åŒºåˆ«", "å“ªäº›", "åˆ—è¡¨", "æ¯”è¾ƒ", "æ‰€æœ‰", "å…¨éƒ¨", "æœ‰ä»€ä¹ˆ",
                        "compare", "difference", "list", " all ", "what are", "which"]

    has_brand = any(brand in q_lower for brand in quick_brands)
    has_keyword = any(kw in q_lower for kw in quick_keywords)
    has_exclude = any(kw in q_lower for kw in exclude_keywords)

    # åªæœ‰åœ¨æ»¡è¶³æ¡ä»¶ä¸”ä¸åŒ…å«æ’é™¤å…³é”®è¯æ—¶æ‰è§¦å‘å¿«é€Ÿè·¯å¾„
    if has_brand and has_keyword and not has_exclude:
        print("  ğŸš€ğŸš€ğŸš€ è§¦å‘è¶…å¿«é€Ÿè·¯å¾„ï¼šç»•è¿‡å·¥ä½œæµï¼Œç›´æ¥æŸ¥è¯¢")
        yield {"node": "fast_path_detected", "status": "activated"}

        # ç”Ÿæˆè¯­ä¹‰æŒ‡çº¹ç”¨äºç¼“å­˜ï¼ˆå³ä½¿èµ°å¿«é€Ÿè·¯å¾„ä¹Ÿè¦ç¼“å­˜ï¼‰
        fingerprint_res = contextualize_question(
            {"question": question, "target_language": "Normalized_Key"})
        semantic_text = fingerprint_res.get("question", "").lower().strip()
        semantic_key = get_semantic_hash(semantic_text)

        print(f"  ğŸ“Œ å¿«é€Ÿè·¯å¾„è¯­ä¹‰æŒ‡çº¹: {semantic_text}")
        print(f"  ğŸ“Œ ç¼“å­˜é”®: {semantic_key}")

        # è°ƒè¯•ï¼šæ‰“å°å½“å‰ç¼“å­˜çŠ¶æ€
        if "response_cache" in st.session_state:
            cache_keys = list(st.session_state.response_cache.keys())
            print(f"  ğŸ” å½“å‰ç¼“å­˜ä¸­æœ‰ {len(cache_keys)} æ¡è®°å½•")
            if len(cache_keys) > 0:
                print(f"  ğŸ” ç¼“å­˜é”®åˆ—è¡¨: {cache_keys[:3]}")  # åªæ‰“å°å‰3ä¸ª

        # æ£€æŸ¥ç¼“å­˜
        if "response_cache" in st.session_state and semantic_key in st.session_state.response_cache:
            print("  âœ“âœ“âœ“ å¿«é€Ÿè·¯å¾„ä¹Ÿå‘½ä¸­äº†è¯­ä¹‰ç¼“å­˜ï¼")
            if "cache_stats" in st.session_state:
                st.session_state.cache_stats["hits"] += 1
            result = st.session_state.response_cache[semantic_key]
            yield {"node": "end", "generation": result, "duration": time.time() - start_time}
            return

        # ç›´æ¥è°ƒç”¨ SQL æŸ¥è¯¢ï¼Œç»•è¿‡æ•´ä¸ªå·¥ä½œæµ
        try:
            result, duration = direct_sql_query_func(question)
            # å­˜å…¥ç¼“å­˜
            if "response_cache" not in st.session_state:
                st.session_state.response_cache = {}
            st.session_state.response_cache[semantic_key] = result
            print(f"  ğŸ’¾ å·²å­˜å…¥è¯­ä¹‰ç¼“å­˜ï¼ˆé”®: {semantic_key}ï¼‰")
            if "cache_stats" in st.session_state:
                st.session_state.cache_stats["misses"] += 1

            yield {"node": "end", "generation": result, "duration": time.time() - start_time}
            return
        except Exception as e:
            print(f"  âš ï¸ å¿«é€Ÿè·¯å¾„å¤±è´¥: {e}ï¼Œå›é€€åˆ°æ­£å¸¸æµç¨‹")
            # å¦‚æœå¤±è´¥ï¼Œç»§ç»­æ­£å¸¸æµç¨‹

    # ğŸš€ å¤šè·³æŸ¥è¯¢çš„å¿«é€Ÿé€šé“ï¼šç›´æ¥è·¯ç”±åˆ° SQL Agentï¼Œè·³è¿‡ route_question
    if has_exclude and has_brand:
        detected_keyword = [k for k in exclude_keywords if k in q_lower][0]
        print(f"  ğŸ¤– æ£€æµ‹åˆ°å¤æ‚æŸ¥è¯¢ï¼ˆåŒ…å«'{detected_keyword}'ï¼‰ï¼Œä½¿ç”¨ Agent å¤šè·³æ¨ç†")
        yield {"node": "complex_query_detected", "keyword": detected_keyword}

        # ğŸš€ ä¼˜åŒ–ï¼šå¯¹äºå¤šè·³æŸ¥è¯¢ï¼Œä¹Ÿæ£€æŸ¥ç¼“å­˜ï¼ˆè·³è¿‡å‰é¢çš„èŠ‚ç‚¹ï¼‰
        fingerprint_res = contextualize_question(
            {"question": question, "target_language": "Normalized_Key"})
        semantic_text = fingerprint_res.get("question", "").lower().strip()
        semantic_key = get_semantic_hash(semantic_text)

        print(f"  ğŸ“Œ å¤šè·³æŸ¥è¯¢è¯­ä¹‰æŒ‡çº¹: {semantic_text}")
        print(f"  ğŸ“Œ ç¼“å­˜é”®: {semantic_key}")

        # æ£€æŸ¥ç¼“å­˜
        if "response_cache" in st.session_state and semantic_key in st.session_state.response_cache:
            print("  âœ“âœ“âœ“ å¤šè·³æŸ¥è¯¢å‘½ä¸­äº†è¯­ä¹‰ç¼“å­˜ï¼è·³è¿‡å®Œæ•´å·¥ä½œæµ")
            if "cache_stats" in st.session_state:
                st.session_state.cache_stats["hits"] += 1
            result = st.session_state.response_cache[semantic_key]
            yield {"node": "end", "generation": result, "duration": time.time() - start_time}
            return

        # ç¼“å­˜æœªå‘½ä¸­ï¼šç”Ÿæˆå±•ç¤ºç”¨çš„è¡¥å…¨æ„å›¾ï¼Œç„¶åç›´æ¥æ‰§è¡Œ SQL Agent
        print("  âœ— å¤šè·³ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œ SQL Agentï¼ˆè·³è¿‡route_questionï¼‰")
        if "cache_stats" in st.session_state:
            st.session_state.cache_stats["misses"] += 1

        display_q_res = contextualize_question(
            {"question": question, "target_language": target_lang})
        refined_q = display_q_res.get("question", question)
        yield {"node": "contextualize_question", "status": "complete", "refined_q": refined_q}

        # ç›´æ¥è°ƒç”¨ SQL Agentï¼ˆè·³è¿‡ route_questionï¼ŒèŠ‚çœ0.5-1ç§’ï¼‰
        yield {"node": "sql_agent", "status": "running"}
        response, _ = sql_query_text(refined_q)

        # å­˜å…¥ç¼“å­˜
        if "response_cache" not in st.session_state:
            st.session_state.response_cache = {}
        st.session_state.response_cache[semantic_key] = response
        print(f"  ğŸ’¾ å·²å­˜å…¥å¤šè·³æŸ¥è¯¢ç¼“å­˜ï¼ˆé”®: {semantic_key}ï¼‰")

        yield {"node": "end", "generation": response, "duration": time.time() - start_time}
        return

    # 4. ç”Ÿæˆè¯­ä¹‰æŒ‡çº¹ä½œä¸ºç¼“å­˜ Key (ç»Ÿä¸€å½’ä¸€åŒ–ä¸ºè‹±æ–‡)
    # è¿™ä¸€æ­¥æ˜¯å…³é”®ï¼šè®©"èƒ½å–å—"å’Œ"Can I drink"åœ¨åå°éƒ½ç”Ÿæˆç›¸åŒçš„è‹±æ–‡å¥å­
    fingerprint_res = contextualize_question(
        {"question": question, "target_language": "Normalized_Key"})
    semantic_text = fingerprint_res.get("question", "").lower().strip()

    # å°†è¯­ä¹‰æ–‡æœ¬è½¬æ¢ä¸ºå“ˆå¸Œï¼Œæå‡ç¼“å­˜é”®æŸ¥æ‰¾æ•ˆç‡
    semantic_key = get_semantic_hash(semantic_text)

    # --- æ‰“å°è§‚å¯Ÿç»“æœ ---
    print(f"\n{'='*20} [SEMANTIC CACHE] {'='*20}")
    print(f"ã€è¯­ä¹‰æŒ‡çº¹ã€‘: {semantic_text}")
    print(f"ã€ç¼“å­˜é”®ã€‘: {semantic_key}")
    print(f"{'='*55}\n")

    # 5. ç”Ÿæˆå±•ç¤ºç”¨çš„è¡¥å…¨æ„å›¾ (ä»…è°ƒç”¨ä¸€æ¬¡ï¼Œé¿å…é‡å¤)
    display_q_res = contextualize_question(
        {"question": question, "target_language": target_lang})
    refined_q = display_q_res.get("question", question)
    yield {"node": "contextualize_question", "status": "complete", "refined_q": refined_q}

    # 6. è¯­ä¹‰çº§ç¼“å­˜æ£€æŸ¥
    if semantic_key in st.session_state.response_cache:
        print("  âœ“âœ“âœ“ å‘½ä¸­è¯­ä¹‰ç¼“å­˜ï¼è·³è¿‡å·¥ä½œæµæ‰§è¡Œ")
        st.session_state.cache_stats["hits"] += 1
        yield {"node": "cache_hit", "status": "complete"}
        final_res = st.session_state.response_cache[semantic_key]
    else:
        # 7. ç¼“å­˜æœªå‘½ä¸­ï¼šæ‰§è¡Œæ­£å¼å·¥ä½œæµ
        print("  âœ— ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œå®Œæ•´å·¥ä½œæµ")
        st.session_state.cache_stats["misses"] += 1
        final_res = "æŠ±æ­‰ï¼Œç”±äºé€»è¾‘å¼‚å¸¸ã€‚"

        # ä¼ å…¥è¡¥å…¨åçš„é—®é¢˜ï¼Œå¹¶è·³è¿‡å·¥ä½œæµå†…éƒ¨çš„é‡å¤è¡¥å…¨èŠ‚ç‚¹
        for event in app.stream({"question": refined_q, "target_language": target_lang, "retry_count": 0}, stream_mode="updates"):
            for node_name, output in event.items():
                if node_name == "contextualize_question":
                    continue
                yield {"node": node_name, "status": "running"}
                if "generation" in output:
                    final_res = output["generation"]

        # å°†ç»“æœå­˜å…¥ç¼“å­˜ (ä½¿ç”¨å“ˆå¸Œé”®)
        st.session_state.response_cache[semantic_key] = final_res

    # 8. æ‰“å°ç¼“å­˜ç»Ÿè®¡
    stats = get_cache_stats()
    print(
        f"ğŸ“Š ç¼“å­˜å‘½ä¸­ç‡: {stats['hit_rate']:.1f}% ({stats['hits']}/{stats['total_queries']})")

    yield {"node": "end", "generation": final_res, "duration": time.time() - start_time}
